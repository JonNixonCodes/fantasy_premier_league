Project
- Fantasy Premier League

Objective
- Data analysis on players, teams, form, past scores

Analytical questions?
- Player stats (last 3/5/10/season)
	- Average points
	- Average goals
	- Average assists
	- Average minutes
	- Average XG/XA per 90
- What are the upcoming fixtures for each player?
- Most transferred players in/out?
- Players in form
- Injury news
- Transfer deadline
- Which players are cost effective?
- Which players are in form?
- Which teams are likely to get clean sheets?
- Which player has difficult fixtures coming up?
- Which player is likely to score the most points for a given gameweek?
- Which player is likely to score bonus points?

Analytical use cases
- Interrogate players/teams/seasons
- Interrogate game weeks

Future Use Cases
- Predicting best players in each position each week
- AI for play FPL

Design
- Cloud Run Job to ingest historical data from API endpoints into JSON files on GCS buckets. Triggered by HTTP request. Ingest into landing bucket (landing/source-name/load-date.json). Add metadata (e.g. source url and load date).
	- Why Cloud Run?
		For this use case I should strongly consider Cloud Functions as it is easier to develop and implement and meets the needs requirement. Benefit of Cloud Run is the flexibility of creating my own containerised application, and the ability to handle multiple requests. Since we are simply running a Python job this is not a requirement. Costs for both solutions are low since we are only running the job daily. For my use case I should remain within Cloud Functions free tier.
- Cloud Run Job to ingest data from API endpoints into JSON files on GCS buckets. Triggered by HTTP request. Triggered daily by Cloud Scheduler job.
	- Why ingest into GCS bucket and not directly into BigQuery?
		Prefer multi-stage data processing as it allows future flexibility since the original raw data is kept. Cheaper to keep it in GCS buckets rather than BigQuery storage. Can keep the data in its original format.
	- Why ingest as JSON files?
		Original format of the input data is JSON.
	- Why use Cloud Scheduler for scheduling job?
		Easiest and cheapest way to do daily scheduling on Google Cloud
- Apache Beam to transform JSON data into parquet files stored using Hive partitioning. Data is copied from landing bucket into archive bucket in case re-processing is required. Transformed data is loaded to staging bucket (data is loaded to failed bucket if there were problems with transformation)
	- Why Apache Beam?
		Enables serverless data processing jobs since the jobs are highly sporadic and don't require resources to be online for long periods of time. Can do parallel processing.
	- Why not use Cloud Functions for data processing
		Cloud functions can only process data in-memory. More difficult to do parallel processing (needs to be hardcoded).
	- Why store as parquet files?
		embedded schema. quicker columnar reads. can be encoded for smaller file size.
	- Why use Hive partitioning?
		gs://myBucket/myTable/source_date=2019-10-31/lang=en/foo
	- Why not directly ingest JSON data into BigQuery?
		Simpler to update the raw input daily as it doesn't require the additional data import step. Simply load the file into the HIVE partitioned GCS bucket.	
- Read data into BigQuery using external tables
	- Why use BigQuery?
		Easier to build dimensional data model using SQL transformations. (E.g. implementing SCD2 in BigQuery)
- Model the data in BigQuery
	- How to model the data?
		Use dimensional modelling. Fact tables: GameWeek and MatchPlayer. SCD2 for Dim tables.
	- Why model the data in this way?
		Simpler and more efficient to build analytical queries (fewer joins). Not as performant as wide fact tables. More difficult to update slow changing dimensions, since many records will need to be updated. Wide table would be more useful as a consumption table for viz and ML.
	- Why not use one wide fact table (with nested (structs) and repeated (arrays) data). Data is pre-joined and therefore more performant. However downside is it mostly enables a very specific query pattern.
	- Should I use SCD2 or SCD1 for my dimension tables?
		- SCD2 is more robust but more complex.
		- Updates to dimensions are rare
		- History for dimension tables is not important
	- How do you deal with new players and teams each season?
		- Having an is current flag

Data model
- Dimensions
	- Team
	- Player
	- Manager
	- seasons
	- Competition
	- Position
- Facts
	- MatchPlayer
	- GameWeek

Further Design Questions
- How to scale up for multiple users?
- How to scale up for multiple data sources?


Project folder structure
.
|
|--setup/
|	|
|	|--cloud_function/
|	|
|	|--cloud_scheduler/
|	|
|	|--data_flow/
|	|
|	|--bigquery/
|
|--test/
|
|--download_historical_data.sh
|
|--download_live_data.sh
|
|--Makefile
|
|--README.md

Questions for Bene
- Tips on data model
- How to implement star schema
- How to create a DAG for queries
- How to create keys
- parquet vs avro

resources
- https://medium.com/happiestneurons/slowly-changing-dimension-type-2-scd2-in-big-query-95330ce31625
- https://kontext.tech/article/670/merge-statement-in-bigquery
- https://rapidapi.com/Wolf1984/api/football-xg-statistics